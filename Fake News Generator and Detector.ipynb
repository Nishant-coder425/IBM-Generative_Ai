{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "L9E04Irr_uHc"
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0dpahTZMKj4y"
   },
   "source": [
    "## **Load GPT-2 for Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5-7QgjezKjKm",
    "outputId": "b52476ce-45c4-4004-9821-539359a79ca4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ Loading GPT-2 model...\n"
     ]
    }
   ],
   "source": [
    "print(\"üîÅ Loading GPT-2 model...\")\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "gpt2_model.config.pad_token_id = gpt2_tokenizer.eos_token_id\n",
    "\n",
    "def generate_fake_news(prompt=\"Breaking News:\"):\n",
    "    if not prompt.strip():\n",
    "        return \"‚ö†Ô∏è No topic provided for generation.\"\n",
    "\n",
    "    input_ids = gpt2_tokenizer.encode(prompt, return_tensors='pt')\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "    outputs = gpt2_model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=30,\n",
    "        num_return_sequences=5,\n",
    "        do_sample=True,\n",
    "        temperature=0.9,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        pad_token_id=gpt2_tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    headlines = []\n",
    "    for output in outputs:\n",
    "        headline = gpt2_tokenizer.decode(output, skip_special_tokens=True)\n",
    "        headlines.append(headline)\n",
    "    return \"\\n\".join([f\"{i+1}. {h}\" for i, h in enumerate(headlines)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iC7IF8y8K4F7"
   },
   "source": [
    "## **Load BERT for Detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qa-RB1jMLCc3",
    "outputId": "7c841bf4-2f50-4194-fcba-4817488a879a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ Loading BERT classifier pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"üîÅ Loading BERT classifier pipeline...\")\n",
    "clf = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"jy46604790/Fake-News-Bert-Detect\",\n",
    "    tokenizer=\"jy46604790/Fake-News-Bert-Detect\"\n",
    ")\n",
    "\n",
    "def detect_fake_news(text):\n",
    "    if not text.strip():\n",
    "        return \"‚ö†Ô∏è No news text provided for detection.\"\n",
    "\n",
    "    result = clf(text[:500])[0]  # truncate long texts\n",
    "    label = \"üü• FAKE\" if result['label'] == \"LABEL_0\" else \"üü© REAL\"\n",
    "    confidence = round(result['score'] * 100, 2)\n",
    "    return f\"üìù Input: {text}\\n\\nüîç Prediction: {label}\\n‚úÖ Confidence: {confidence}%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NcLIhdJMLLiG"
   },
   "source": [
    "## **Gradio Interface Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "4vNhi201Lbt-"
   },
   "outputs": [],
   "source": [
    "def interface(prompt, text):\n",
    "    fake_news = generate_fake_news(prompt) if prompt.strip() else \"‚ö†Ô∏è No topic entered.\"\n",
    "    detection = detect_fake_news(text) if text.strip() else \"‚ö†Ô∏è No news text entered.\"\n",
    "    return fake_news, detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4iQ7Ls_tLewx"
   },
   "source": [
    "## **Gradio App**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bJUogapQLj2b",
    "outputId": "2d390e67-eaad-47a1-8c86-c4797b47010b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/gradio/interface.py:416: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "demo = gr.Interface(\n",
    "    fn=interface,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"üß† Enter a Topic for Fake News\", placeholder=\"e.g. Budget 2025, Alien Invasion\"),\n",
    "        gr.Textbox(label=\"üìÑ Paste News to Detect\", placeholder=\"e.g. Government announces free petrol every Monday...\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"üì∞ Generated Fake News\"),\n",
    "        gr.Textbox(label=\"üïµÔ∏è‚Äç‚ôÇÔ∏è Detection Result\")\n",
    "    ],\n",
    "    title=\" Fake News Generator & Detector using Generative AI and NLP\",\n",
    "    description=\"\\n\",\n",
    "    allow_flagging=\"never\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WmV2jtJzQEts"
   },
   "outputs": [],
   "source": [
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Viv15FdQFxf"
   },
   "source": [
    "![Image of output](image1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpNxXft_QGP6"
   },
   "source": [
    "![Image of Output](image2.png)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
